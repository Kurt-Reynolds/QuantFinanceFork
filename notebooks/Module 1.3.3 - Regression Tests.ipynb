{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Quantitative Finance\n",
    "\n",
    "Copyright (c) 2019 Python Charmers Pty Ltd, Australia, <https://pythoncharmers.com>. All rights reserved.\n",
    "\n",
    "<img src=\"img/python_charmers_logo.png\" width=\"300\" alt=\"Python Charmers Logo\">\n",
    "\n",
    "Published under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. See `LICENSE.md` for details.\n",
    "\n",
    "Sponsored by Tibra Global Services, <https://tibra.com>\n",
    "\n",
    "<img src=\"img/tibra_logo.png\" width=\"300\" alt=\"Tibra Logo\">\n",
    "\n",
    "\n",
    "## Module 1.3: Ordinary Least Squares\n",
    "\n",
    "### 1.3.2 Regression Tests\n",
    "\n",
    "In this module we will look further into Multivariate OLS and examine some of the requirements of the algorithm, as well as some of the details of the regression results we saw in the last module.\n",
    "\n",
    "When performing OLS for Linear Regression Models, there are a few assumptions that need to be met. The key ones are:\n",
    "\n",
    "The first assumption is the key one - that is that the relationship between $X$ and $Y$ can, in fact, be described using the model $Y = X\\beta + u$. It may *not* be able to be precisely modeled this way, but it may be possible to get close enough that it doesn't matter.\n",
    "\n",
    "The second assumption is that the expected value of $u$ is zero. There may be fluctuations in the vector $u$, but the overall expected value is 0. More formally, we assume that $E(u|X) = 0$, that is the expected value of $u$ when given $X$ is zero. If it were not, then we can alter the bias term to make it zero, which would be learned from the OLS, giving us our zero value!\n",
    "\n",
    "The third assumption is that the error term ($u$) and the data itself $X$ do not have any correlation. In other words, $u$ is unexplained error that cannot be explained by the data. Put more formally, there is no hetroskedasticity or autocorrelation between $u$ and $X$, which is a stronger assumption than the second, but along the same lines. We will cover these terms in a later module more formally.\n",
    "\n",
    "The fourth assumption is that $X$ has a finite variance. This is sometimes (slightly incorrectly) referred to as $X$ being non-stochastic. We will investigate how variance plays into the model in several later modules.\n",
    "\n",
    "The fifth assumption is that there are no linear relation between the measurements (variables, columns, features) in $X$, known as having **full column rank**.\n",
    "\n",
    "If any of these assumptions are untrue, the resulting model does not necessarily have the properties we will discuss in the rest of this module, and the model itself might be biased or inaccurate. However, it may still be *useful* in a practical sense. For instance, if two variables are slightly linearly related, we break the last assumption, however in practice the model is generally still useful. However if they are heavily related, then the resulting model will be unstable.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    Like most models and concepts, there is always some debate about the definitions and assumptions behind them. Further, some people use the same term to describe different concepts. When discussing an algorithm, it would be best practice to note any key assumptions or variance from the \"norm\" that you consider. If you aren't sure, provide a reference.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in some data for a regression problem and have a look at the results. In this dataset, we are trying to predict house prices from other characteristics of the area, in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a dataset from the scikit learn repository\n",
    "# scikit-learn is a machine learning library, and has a few sample datasets \n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cali_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cali_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_to_df(sklearn_dataset):\n",
    "    # A helper function to convert the scikit-learn dataset to a pandas DataFrame\n",
    "    # From: https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset/46379878#46379878\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali = sklearn_to_df(cali_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cali.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "est = smf.ols(formula='target ~ MedInc + HouseAge + AveRooms + AveBedrms + Population + AveOccup + Latitude + Longitude', \n",
    "              data=cali).fit()  # Does the constant for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared:         </th> <td>   0.606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3970.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 04 Sep 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:44:35</td>     <th>  Log-Likelihood:    </th> <td> -22624.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 20640</td>      <th>  AIC:               </th> <td>4.527e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 20631</td>      <th>  BIC:               </th> <td>4.534e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>  -36.9419</td> <td>    0.659</td> <td>  -56.067</td> <td> 0.000</td> <td>  -38.233</td> <td>  -35.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MedInc</th>     <td>    0.4367</td> <td>    0.004</td> <td>  104.054</td> <td> 0.000</td> <td>    0.428</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HouseAge</th>   <td>    0.0094</td> <td>    0.000</td> <td>   21.143</td> <td> 0.000</td> <td>    0.009</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveRooms</th>   <td>   -0.1073</td> <td>    0.006</td> <td>  -18.235</td> <td> 0.000</td> <td>   -0.119</td> <td>   -0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveBedrms</th>  <td>    0.6451</td> <td>    0.028</td> <td>   22.928</td> <td> 0.000</td> <td>    0.590</td> <td>    0.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Population</th> <td>-3.976e-06</td> <td> 4.75e-06</td> <td>   -0.837</td> <td> 0.402</td> <td>-1.33e-05</td> <td> 5.33e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveOccup</th>   <td>   -0.0038</td> <td>    0.000</td> <td>   -7.769</td> <td> 0.000</td> <td>   -0.005</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Latitude</th>   <td>   -0.4213</td> <td>    0.007</td> <td>  -58.541</td> <td> 0.000</td> <td>   -0.435</td> <td>   -0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Longitude</th>  <td>   -0.4345</td> <td>    0.008</td> <td>  -57.682</td> <td> 0.000</td> <td>   -0.449</td> <td>   -0.420</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4393.650</td> <th>  Durbin-Watson:     </th> <td>   0.885</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>14087.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.082</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 6.420</td>  <th>  Cond. No.          </th> <td>2.38e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.38e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      target      & \\textbf{  R-squared:         } &     0.606   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.606   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     3970.   \\\\\n",
       "\\textbf{Date:}             & Mon, 04 Sep 2023 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     09:44:35     & \\textbf{  Log-Likelihood:    } &   -22624.   \\\\\n",
       "\\textbf{No. Observations:} &       20640      & \\textbf{  AIC:               } & 4.527e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &       20631      & \\textbf{  BIC:               } & 4.534e+04   \\\\\n",
       "\\textbf{Df Model:}         &           8      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}  &     -36.9419  &        0.659     &   -56.067  &         0.000        &      -38.233    &      -35.650     \\\\\n",
       "\\textbf{MedInc}     &       0.4367  &        0.004     &   104.054  &         0.000        &        0.428    &        0.445     \\\\\n",
       "\\textbf{HouseAge}   &       0.0094  &        0.000     &    21.143  &         0.000        &        0.009    &        0.010     \\\\\n",
       "\\textbf{AveRooms}   &      -0.1073  &        0.006     &   -18.235  &         0.000        &       -0.119    &       -0.096     \\\\\n",
       "\\textbf{AveBedrms}  &       0.6451  &        0.028     &    22.928  &         0.000        &        0.590    &        0.700     \\\\\n",
       "\\textbf{Population} &   -3.976e-06  &     4.75e-06     &    -0.837  &         0.402        &    -1.33e-05    &     5.33e-06     \\\\\n",
       "\\textbf{AveOccup}   &      -0.0038  &        0.000     &    -7.769  &         0.000        &       -0.005    &       -0.003     \\\\\n",
       "\\textbf{Latitude}   &      -0.4213  &        0.007     &   -58.541  &         0.000        &       -0.435    &       -0.407     \\\\\n",
       "\\textbf{Longitude}  &      -0.4345  &        0.008     &   -57.682  &         0.000        &       -0.449    &       -0.420     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 4393.650 & \\textbf{  Durbin-Watson:     } &     0.885  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 14087.596  \\\\\n",
       "\\textbf{Skew:}          &   1.082  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &   6.420  & \\textbf{  Cond. No.          } &  2.38e+05  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.38e+05. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   R-squared:                       0.606\n",
       "Model:                            OLS   Adj. R-squared:                  0.606\n",
       "Method:                 Least Squares   F-statistic:                     3970.\n",
       "Date:                Mon, 04 Sep 2023   Prob (F-statistic):               0.00\n",
       "Time:                        09:44:35   Log-Likelihood:                -22624.\n",
       "No. Observations:               20640   AIC:                         4.527e+04\n",
       "Df Residuals:                   20631   BIC:                         4.534e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -36.9419      0.659    -56.067      0.000     -38.233     -35.650\n",
       "MedInc         0.4367      0.004    104.054      0.000       0.428       0.445\n",
       "HouseAge       0.0094      0.000     21.143      0.000       0.009       0.010\n",
       "AveRooms      -0.1073      0.006    -18.235      0.000      -0.119      -0.096\n",
       "AveBedrms      0.6451      0.028     22.928      0.000       0.590       0.700\n",
       "Population -3.976e-06   4.75e-06     -0.837      0.402   -1.33e-05    5.33e-06\n",
       "AveOccup      -0.0038      0.000     -7.769      0.000      -0.005      -0.003\n",
       "Latitude      -0.4213      0.007    -58.541      0.000      -0.435      -0.407\n",
       "Longitude     -0.4345      0.008    -57.682      0.000      -0.449      -0.420\n",
       "==============================================================================\n",
       "Omnibus:                     4393.650   Durbin-Watson:                   0.885\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14087.596\n",
       "Skew:                           1.082   Prob(JB):                         0.00\n",
       "Kurtosis:                       6.420   Cond. No.                     2.38e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.38e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, there is a coef column, which gives the values for $\\beta$ in our model for each independent variable.\n",
    "If the coefficient is negative, there is an inverse relationship between the independent variable and the dependent one.\n",
    "It is important to note that this is not a direct relationship, as retraining the model with just one parameter will likely change this coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "est_simple = smf.ols(formula='target ~ MedInc', \n",
    "              data=cali).fit()  # Does the constant for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    0.450856\n",
       "MedInc       0.417938\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_simple.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the coefficient itself, we are given the standard error, the probability (using the t-statistic) that this value is significant (i.e. if it is less than 0.05), and the lower and upper bounds for the 95% confidence interval - where we can say with 95% confidence that the true value lies within those bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key reason for this is related to the second warning, indicating there is a strong multicollinearity. We will review this term in the next module and fix the problem it is causing over the next few modules. For now, it indicates that the independent variables are effectively correlated to a high degree, which breaks an assumption with OLS. In short, it means the independent variables are each predicting the same components of the output, and the coefficients are effectively arbitrary. \n",
    "\n",
    "As an example, if we have two variables $a$ and $b$ that are correlated, the coefficient value for $a$ and $b$ in a trained model is effectively shared between them, and whatever value actually appears in the OLS model is just one of many possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test statistics, good values (for various definitions of \"good\") for these scores allow us to say with a high confidence that the model accurately predicts the data. Bad values indicate that the model should not be used. We will now review a few key values from this table, as a means to validate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $R^2$ statistic\n",
    "\n",
    "The key statistic to review, and the \"one value\" that you are likely to report in your executive summary, is the $R^2$ statistic. It measures how much of the variance in the predicted variable ($Y$) is explained by your model ($X\\beta$), compared to the error of the model ($u$). A high value (near 1) indicates that the model perfectly explains the variable being predicted. A low value (near 0) indicates that the model does not explain the variable at all, which is achieved if the model always predicts the expected value of $Y$. The score can be negative as well, as the model itself can be a net-negative in predictive power (i.e. it model actually predicts incorrectly more than correctly).\n",
    "\n",
    "In the above results, the $R^2$ value is around 0.606, indicating that around 61% of the variance in the predicted variable $Y$ can be explained by the model $X\\beta$. That said, our model has a few problems which we will address soon.\n",
    "\n",
    "To obtain the $R^2$ value, store the regression results object obtained above and extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6062326851998051"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "1. Review the documentation at the following link to see what other values can be obtained from a trained estimator:     http://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html#statsmodels.regression.linear_model.RegressionResults\n",
    "2. What is the difference between `est.rsquared` and `est.rsquared_adj`? When should you use one over the other?\n",
    "\n",
    "There are quite a few terms on the documentation page we haven't seen yet - many will be reviewed in later modules in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant in a regression model. In other words, the adjusted R-squared shows whether adding additional predictors improve a regression model or not.\n",
    "\n",
    " - from https://corporatefinanceinstitute.com/resources/knowledge/other/adjusted-r-squared/\n",
    " \n",
    "In general I would prefer the adjusted R-squared unless there is a very simple multiple with a small number of predictors which are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $F$ statistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F$ statistic is another measure of how significant the fit is. It divides the mean squared error of the model, by the mean squared error of the error term in the model. The probability value under it indicates the probability that we would achieve such a statistic, *if all the coefficients were zero*. In our model, our probability is very high (3970) indicating there is almost a high chance that such an F statistic would be obtained by such a \"zero\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3970.3608128011997"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.fvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.f_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put this formally, the F statistic is a test against the null hypothesis:\n",
    "\n",
    "$H_0: \\beta_i = 0 \\forall i$\n",
    "\n",
    "The alternative hypothesis is that *at least* one of the values in $\\beta$ is not 0.\n",
    "\n",
    "The F statistic can be computed using the following terms:\n",
    "\n",
    "$ F = \\frac{ESS}{RSS}$\n",
    "\n",
    "Where $ESS$ is the explained variance of the model and $RSS$ is the unexplained variance. Given the explained variance of the model is due to the component $\\beta X$ and the unexplained component is due to $u$, we can derive the equations as below:\n",
    "\n",
    "$ ESS = \\frac{1}{k-1}\\sum{(\\hat{Y_i} - \\bar{Y})^2}$\n",
    "\n",
    "Where $\\hat{Y_i}$ is the *ith* predicted value and $\\bar{Y}$ is the overall mean of $Y$, and $k$ is the number of variables. In other words, it is the total deviation from the mean that the model explains.\n",
    "\n",
    "For the variance explained by the residuals, we get:\n",
    "\n",
    "$ RSS = \\frac{n}{k}\\sum{u_i^2}$\n",
    "\n",
    "Where $u$ is the error term in our linear regression model and $n$ is the number of samples. There are a few ways to alter these equations to make them easier to compute, all based on performing algebra with the OLS estimator equations defined in earlier modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Function, Akaike information criterion (AIC) and  Bayesian information criterion (BIC)\n",
    "\n",
    "These three measures are related, and represent the plausibility of the given data given the set of parameters in the model.\n",
    "In all three cases, we use them as relative values. That is, we use these values to compare two different models, and choose the model with the lowest score of these three values (or whichever single statistic you are most concerned with).\n",
    "\n",
    "For instance, if model 1 has a BIC of 3085 and model 2 has a BIC of 4000, we choose model 1.\n",
    "\n",
    "The key function here is the likelihood function, which is used to compute the AIC and BIC. The likelihood function $\\mathcal{L}(\\beta \\mid x)$ is the likelihood that the data could be generated from a model with the given parameters. From an information theory perspective, we aim to maximise the likelihood function. From a computing perspective, it is often easier to both compute the *log likelihood*, and to *minimise the negative log likelihood*. A key component of this is that computers find adding numbers easier than multiplying small numbers, and we can convert from log-space to normal-space using the following pattern:\n",
    "\n",
    "$log(xy) = log(x) + log(y)$\n",
    "\n",
    "When dealing with probabilities, many probability values are very small, and multiplying small numbers near zero is hard for computers. Often, they \"underflow\" and consider a very small number to just be zero, and then any product from that point on is zero. Instead, we compute the log of all numbers and add them together - no underflow!\n",
    "\n",
    "Once the likelihood function (or negative log likelihood) has been computed, the maximum value it can take (when optimised) is $\\hat{L}$. From here, the AIC is defined as:\n",
    "\n",
    "$ AIC = sk - s\\ln(\\hat{L})$\n",
    "\n",
    "The BIC is defined similarly:\n",
    "\n",
    "$ BIC = \\ln(n)k - 2\\ln({\\hat L})$\n",
    "\n",
    "Typically the BIC is preferred, as it is more stable in most circumstances. However, for the BIC to be valid, the number of samples must be much more than the number of parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
